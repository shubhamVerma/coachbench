<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LLM Reflective Questioning Benchmark</title>
  <link rel="stylesheet" href="css/tufte.css">
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/marked@11.1.1/marked.min.js"></script>
  
  <!-- JS Modules -->
  <script src="js/config.js"></script>
  <script src="js/data-loader.js"></script>
  <script src="js/chat-renderer.js"></script>
  <script src="js/tab-manager.js"></script>
  <script src="js/utils.js"></script>

  <!-- Initialization -->
  <script>
document.addEventListener('DOMContentLoaded', () => {
  console.log('Initializing interactive evaluations...');

  DataLoader.loadResponsesIndex()
    .then(() => {
      TabManager.setupTabHandlers();

      // Load first scenario
      const firstCategory = TabManager.getCurrentCategory();
      if (firstCategory) {
        const scenarios = TabManager.getScenariosByCategory(firstCategory);
        if (scenarios && scenarios.length > 0) {
          DataLoader.loadScenario(scenarios[0].id);
        }
      }
    })
    .catch(error => {
      console.error('Failed to initialize:', error);
      const evalContent = document.getElementById('eval-content');
      evalContent.innerHTML = '<p style="text-align: center; color: var(--accent-color);">Failed to load scenarios</p>';
    });
});
  </script>
</head>
<body>
  <div class="container">
    <header>
      <h1>LLM Reflective Questioning Benchmark</h1>
      <p class="subtitle">An Evaluation of Coaching-Style Communication in AI Models</p>
      <p style="margin-top: 1rem; font-family: var(--font-sans); font-size: 0.9rem; color: var(--muted-color);">
        by Shubham Verma
      </p>
    </header>

    <section id="abstract">
      <h2>Abstract</h2>
      <div class="abstract">
        <p>
          This benchmark evaluates the quality of coaching-style communication across three leading large language models (Claude Sonnet 4.5, ChatGPT 4o Mini, and Gemini 3 Flash) using 42 personal growth scenarios. Each model engaged in 3-turn conversations, producing a total of 126 interactions. Independent evaluation using DeepSeek-V3 assessed performance across six dimensions based on ICF Core Competencies. Claude Sonnet 4.5 achieved the highest overall score (16.4/30), demonstrating statistically significant superiority in reflective questioning techniques compared to other models.
        </p>
      </div>
    </section>

    <section id="experiment-setup">
      <h2>Experiment Setup</h2>

      <h3>Models Evaluated</h3>
      <ul>
        <li><strong>Anthropic: Claude Sonnet 4.5</strong> (Free Web via claude.ai)</li>
        <li><strong>OpenAI: ChatGPT 4o Mini</strong> (Free Web via chat.openai.com)</li>
        <li><strong>Google: Gemini 3 Flash Preview</strong> (Free Web via gemini.google.com)</li>
      </ul>

      <h3>Methodology</h3>
      <div class="stats-box">
        <p><strong>Total Scenarios:</strong> 42 personal growth scenarios</p>
        <p><strong>Scenario Categories:</strong> 6 categories (career transitions, relationship patterns, identity perception, decision making, habit formation, motivation resistance)</p>
        <p><strong>Conversation Structure:</strong> 3-turn conversations with each model</p>
        <p><strong>Total Evaluations:</strong> 126 (42 scenarios × 3 models)</p>
        <p><strong>Evaluation Method:</strong> Independent assessment using DeepSeek-V3</p>
        <p><strong>Scoring Framework:</strong> Based on ICF Core Competencies</p>
        <p><strong>Focus:</strong> Reflective questioning vs advice-giving</p>
      </div>

      <h3>Evaluation Dimensions</h3>
      <p>Each conversation was scored across six dimensions (0-5 points each, total 30 points):</p>
      <ul>
        <li><strong>Evokes Awareness:</strong> Ability to help clients discover insights</li>
        <li><strong>Active Listening Indicators:</strong> Presence of reflective and clarifying statements</li>
        <li><strong>Maintains Client Agency:</strong> Avoiding directive advice or solution-giving</li>
        <li><strong>Question Depth Progression:</strong> Movement from surface to deeper inquiry</li>
        <li><strong>Client-Centered Communication:</strong> Focus on client's perspective and experience</li>
        <li><strong>Ethical Boundaries:</strong> Appropriate scope and professional conduct</li>
      </ul>
    </section>

    <section id="data-setup">
      <h2>Data Setup</h2>

      <div id="eval-content">
        <div class="loading">Loading scenarios...</div>
      </div>
    </section>

    <section id="results-summary">
      <h2>Results Summary</h2>

      <h3>Overall Rankings</h3>
      <table>
        <thead>
          <tr>
            <th>Rank</th>
            <th>Model</th>
            <th>Overall Score</th>
            <th>Evokes Awareness</th>
            <th>Active Listening</th>
            <th>Maintains Agency</th>
            <th>Question Depth</th>
            <th>Client-Centered</th>
            <th>Ethical Boundaries</th>
          </tr>
        </thead>
        <tbody id="rankings-table"></tbody>
      </table>

      <div class="key-finding">
        <p><strong>Best Overall Performance:</strong> Claude Sonnet 4.5 achieved the highest overall score of 16.4/30, demonstrating statistically significant superiority in reflective questioning techniques compared to other models.</p>
      </div>

      <h3>Key Findings</h3>
      <div class="key-finding">
        <p><strong>Strongest Dimension:</strong> All models performed best on Active Listening Indicators, with Claude leading at 3.4/5.</p>
      </div>
      <div class="key-finding">
        <p><strong>Challenging Dimension:</strong> Maintains Client Agency proved most difficult across all models, with Gemini scoring lowest at 1.2/5, indicating a tendency toward directive advice.</p>
      </div>

      <h3>Statistical Significance Analysis</h3>
      <div class="stats-box">
        <p><strong>Claude vs ChatGPT:</strong> 3.36 point gap (11.2% of total score) — Highly significant</p>
        <p><strong>Claude vs Gemini:</strong> 5.00 point gap (16.7% of total score) — Extremely significant</p>
        <p><strong>ChatGPT vs Gemini:</strong> 1.64 point gap (5.5% of total score) — Marginally significant</p>
      </div>

      <h3>Visualizations</h3>
      <div class="visualization">
        <div style="max-width: 900px; margin: 0 auto;">
          <canvas id="overallScoresChart"></canvas>
        </div>
        <div class="figcaption">Figure 1: Overall model performance comparison</div>

      <div class="visualization">
        <div style="max-width: 900px; margin: 0 auto;">
          <canvas id="dimensionBreakdownChart"></canvas>
        </div>
        <div class="figcaption">Figure 2: Performance breakdown by evaluation dimension</div>

      <div class="visualization">
        <div style="max-width: 900px; margin: 0 auto;">
          <canvas id="coachingVsAdviceChart"></canvas>
        </div>
        <div class="figcaption">Figure 3: Coaching-style vs advice-giving tendencies</div>

      <div class="visualization">
        <div style="max-width: 900px; margin: 0 auto;">
          <canvas id="statisticalSignificanceChart"></canvas>
        </div>
        <div class="figcaption">Figure 4: Statistical significance with standard deviation error bars</div>
      </div>
    </section>

    <section id="interactive-evaluations">
      <h2>Interactive Evaluations Exploration</h2>

      <div class="tabs-container">
        <div class="tabs" id="eval-category-tabs">
          <button class="tab active" data-category="career_transitions">Career Transitions</button>
          <button class="tab" data-category="relationship_patterns">Relationship Patterns</button>
          <button class="tab" data-category="identity_perception">Identity Perception</button>
          <button class="tab" data-category="decision_making">Decision Making</button>
          <button class="tab" data-category="habit_formation">Habit Formation</button>
          <button class="tab" data-category="motivation_resistance">Motivation Resistance</button>
        </div>

        <div id="eval-content">
          <div class="loading">Loading scenarios...</div>
        </div>
      </section>

    <section id="conclusion">
      <h2>Conclusion</h2>

      <p>
        This benchmark reveals significant differences in coaching-style communication capabilities among leading LLMs. Claude Sonnet 4.5 demonstrates statistically superior performance, particularly in active listening and question depth progression. All models show strengths in reflective communication tendencies but struggle with maintaining client agency, often defaulting to directive advice.
      </p>

      <h3>Practical Implications</h3>
      <p><strong>High-Scoring Models (Claude) are well-suited for:</strong></p>
      <ul>
        <li>Self-reflection and journaling prompts</li>
        <li>Thought partnership for decision-making</li>
        <li>Learning coaching-style communication techniques</li>
        <li>Exploring ideas without receiving directive advice</li>
      </ul>

      <p><strong>When Human Coaching Remains Essential:</strong></p>
      <ul>
        <li>Sustained personal development work</li>
        <li>Accountability partnerships</li>
        <li>Processing complex emotions</li>
        <li>Leadership development</li>
        <li>Life transitions requiring relational support</li>
      </ul>

      <h3>Important Limitations</h3>
      <ul>
        <li><strong>Not evaluating actual coaching:</strong> This measures coaching-style communication techniques, not embodied coaching relationships</li>
        <li><strong>Text-only evaluation:</strong> Cannot assess presence, intuition, or relational dynamics</li>
        <li><strong>Simulated conversations:</strong> Turn 3 responses are AI-generated, not from real humans</li>
        <li><strong>Specific scenarios:</strong> Results may vary with different types of challenges or conversation contexts</li>
      </ul>
    </section>

    <section id="about-author">
      <h2>About Author</h2>
      <p>
        <strong>Shubham Verma</strong> is a researcher and developer interested in the intersection of artificial intelligence and personal development. This benchmark was created to evaluate how well current AI models can engage in coaching-style reflective questioning, and to provide insights for both AI developers and coaching practitioners.
      </p>
    </section>

    <footer>
      <p>LLM Reflective Questioning Benchmark • Generated January 2026</p>
      <p style="font-size: 0.85rem; margin-top: 0.5rem;">Evaluation based on ICF Core Competencies • Analysis using DeepSeek-V3</p>
    </footer>
  </div>

  <script>
const MODEL_CONFIG = {
  claude: { name: 'Claude Sonnet 4.5', color: '#2c5282' },
  chatgpt: { name: 'ChatGPT 4o Mini', color: '#c05621' },
  gemini: { name: 'Gemini 3 Flash', color: '#276749' }
};

const TabManager = {
  currentCategory: null,

  setupTabHandlers() {
    console.log('TabManager: Setting up tab handlers...');

    const tabs = document.querySelectorAll('#eval-category-tabs .tab');

    if (tabs.length === 0) {
      console.error('TabManager: No category tabs found!');
      return;
    }

    tabs.forEach(tab => {
      tab.addEventListener('click', () => {
        console.log('TabManager: Tab clicked:', tab.dataset.category);

        const category = tab.dataset.category;

        tabs.forEach(t => t.classList.remove('active'));
        tab.classList.add('active');

        const scenarios = window.DataLoader.getScenariosByCategory(category);
        if (scenarios && scenarios.length > 0) {
          window.DataLoader.loadScenario(scenarios[0].id);
        }
      });
    });

    console.log('TabManager: Tab handlers setup complete');
  },

  getCurrentCategory() {
    const activeTab = document.querySelector('#eval-category-tabs .tab.active');
    return activeTab ? activeTab.dataset.category : null;
  },

  getCurrentIndex() {
    const category = this.getCurrentCategory();
    if (!category) return -1;

    const scenarios = window.DataLoader.getScenariosByCategory(category);
    const currentScenario = window.ChatRenderer.getCurrentScenario();
    if (!currentScenario) return -1;

    return scenarios.findIndex(s => s.id === currentScenario.id);
  },

  getScenariosByCategory(category) {
    return window.DataLoader.getScenariosByCategory(category);
  }
};

const ChatRenderer = {
  currentScenario: null,
  currentResponses: null,

  getScenariosByCategory(category) {
    return window.DataLoader.getScenariosByCategory(category);
  },

  initialize() {
    console.log('ChatRenderer: Initialized');
  },

  renderChatComparison(scenario, responses) {
    console.log('ChatRenderer: Rendering chat comparison for scenario:', scenario.id);

    this.currentScenario = scenario;
    this.currentResponses = responses;

    const container = document.getElementById('eval-content');
    if (!container) {
      console.error('ChatRenderer: eval-content container not found');
      return;
    }

    const [claude, chatgpt, gemini] = responses;

    const models = [
      { key: 'claude', name: MODEL_CONFIG.claude.name, color: MODEL_CONFIG.claude.color, data: claude },
      { key: 'chatgpt', name: MODEL_CONFIG.chatgpt.name, color: MODEL_CONFIG.chatgpt.color, data: chatgpt },
      { key: 'gemini', name: MODEL_CONFIG.gemini.name, color: MODEL_CONFIG.gemini.color, data: gemini }
    ];

    const html = `
      <div class="scenario-comparison-header">
        <div class="scenario-info">
          <span class="scenario-id">${scenario.id}</span>
          <p class="scenario-prompt">${this.escapeHtml(scenario.prompt)}</p>
        </div>
        <div class="nav-controls">
          <button class="nav-button" id="prev-scenario">← Previous</button>
          <span class="scenario-counter" id="scenario-counter">1 / 7</span>
          <button class="nav-button" id="next-scenario">Next →</button>
        </div>
      </div>

      <div class="chat-comparison-container">
    `;

    models.forEach(model => {
      html += `
          <div class="chat-column" id="${model.key}-column">
            <div class="model-header" style="background: ${model.color};">
              <span class="model-name">${model.name}</span>
              <span class="model-metrics" id="${model.key}-metrics"></span>
            </div>
            <div class="chat-messages" id="${model.key}-messages"></div>
          </div>
        `;
    });

    html += '</div>';

    container.innerHTML = html;

    models.forEach(model => {
      const data = model.data;

      this.renderMessage(model.key, data.turn1.content, data.turn1);

      this.renderUserMessage(model.key, data.turn2_user_response || 'No user response recorded');

      this.renderMessage(model.key, data.turn2.content, data.turn2);

      this.renderUserMessage(model.key, data.turn3_user_response || 'No user response recorded');

      this.renderMessage(model.key, data.turn3.content, data.turn3);

      this.updateModelMetrics(model.key, data.turn1, data.turn2, data.turn3);
    });

    console.log('ChatRenderer: Chat comparison rendered successfully');
  },

  renderMessage(modelKey, content, metadata) {
    const container = document.getElementById(`${modelKey}-messages`);
    if (!container) {
      console.error(`ChatRenderer: Container ${modelKey}-messages not found`);
      return;
    }

    console.log(`ChatRenderer: Rendering message for ${modelKey}`);

    const messageDiv = document.createElement('div');
    messageDiv.className = 'message';

    const contentDiv = document.createElement('div');
    contentDiv.className = 'message-content';
    contentDiv.innerHTML = marked.parse(content);

    messageDiv.appendChild(contentDiv);
    container.appendChild(messageDiv);

    if (metadata && metadata.usage) {
      const time = (metadata.response_time_ms / 1000).toFixed(1) + 's';
      const tokens = metadata.usage.total_tokens || 'N/A';
      this.updateModelMetrics(modelKey, time, tokens);
    }
  },

  renderUserMessage(modelKey, content) {
    const container = document.getElementById(`${modelKey}-messages`);
    if (!container) {
      console.error(`ChatRenderer: Container ${modelKey}-messages not found for user message`);
      return;
    }

    const userMessageDiv = document.createElement('div');
    userMessageDiv.className = 'user-message-inline';
    userMessageDiv.innerHTML = `
      <span class="user-message-label">User Response</span>
      <p style="margin: 0.5rem 0 0 0;">${this.escapeHtml(content)}</p>
    `;

    const chatContainer = document.querySelector('.chat-comparison-container');
    chatContainer.parentNode.insertBefore(userMessageDiv, chatContainer);
  },

  updateModelMetrics(modelKey, turn1, turn2, turn3) {
    const metricsEl = document.getElementById(`${modelKey}-metrics`);
    if (!metricsEl) {
      console.error(`ChatRenderer: Metrics element ${modelKey}-metrics not found`);
      return;
    }

    if (!turn1 || !turn2 || !turn3) {
      console.error('ChatRenderer: Missing turn data for', modelKey);
      return;
    }

    const totalTime = ((turn1.response_time_ms || 0) + (turn2.response_time_ms || 0) + (turn3.response_time_ms || 0)) / 1000;
    const totalTokens = (turn1.usage?.total_tokens || 0) + (turn2.usage?.total_tokens || 0) + (turn3.usage?.total_tokens || 0);

    const time = totalTime.toFixed(1) + 's';
    const tokens = totalTokens || 'N/A';

    console.log(`ChatRenderer: Setting metrics for ${modelKey}: ${time} (${tokens} tokens)`);
    metricsEl.textContent = `${time} (${tokens} tokens)`;
  },

  escapeHtml(text) {
    const div = document.createElement('div');
    div.textContent = text;
    return div.innerHTML;
  },

  getCurrentScenario() {
    return this.currentScenario;
  },

  getCurrentResponses() {
    return this.currentResponses;
  }
};

const Utils = {
  escapeHtml(text) {
    const div = document.createElement('div');
    div.textContent = text;
    return div.innerHTML;
  },

  formatTime(seconds) {
    return seconds.toFixed(1) + 's';
  },

  formatTokens(tokens) {
    return tokens || 'N/A';
  }
};
</script>
</body>
</html>