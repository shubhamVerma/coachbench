# LLM Reflective Questioning Benchmark - Results

## Overview

- **Total Scenarios**: 42
- **Total Evaluations**: N/A
- **Evaluation Date**: 2026-02-01 12:30:33
        - **Methodology**: 3 runs per evaluation at temperature=0 for reproducibility

## Model Rankings

| Rank | Model | Overall Score | Evokes Awareness | Active Listening | Maintains Agency | Question Depth | Client-Centered | Ethical Boundaries |
|------|-------|---------------|------------------|------------------|------------------|----------------|-----------------|-------------------|
| 1 | Anthropic: Claude Sonnet-4.5 (Free Web via claude.ai) | 16.5 ± 0.37 | 2.9 | 3.4 | 2.1 | 3.1 | 2.9 | 2.2 |
| 2 | OpenAI: GPT-5.2 Chat (Free Web via chat.openai.com) | 14.7 ± 0.38 | 2.3 | 3.1 | 1.9 | 2.5 | 2.7 | 2.0 |
| 3 | Mistral AI: Large (Powers Le Chat web/iOS/Android) | 12.1 ± 0.20 | 2.0 | 2.5 | 1.3 | 1.9 | 2.3 | 2.0 |
| 4 | Google: Gemini 3 Flash Preview (Free Web via gemini.google.com) | 11.6 ± 0.28 | 2.0 | 2.2 | 1.2 | 2.0 | 2.1 | 1.9 |
| 5 | xAI: Grok 4.1 Fast (Default user-facing via grok.com, X, mobile apps) | 10.9 ± 0.23 | 1.9 | 2.3 | 1.2 | 1.6 | 2.1 | 1.9 |

## Key Findings

### Best Overall Performance

**Anthropic: Claude Sonnet-4.5 (Free Web via claude.ai)** achieved the highest overall score of 16.5/30.

Key strengths:
- Evokes Awareness: 2.9/5
- Active Listening: 3.4/5
- Maintains Client Agency: 2.1/5

## Methodology

This benchmark evaluates coaching-style communication quality in LLMs using:
- 42 personal growth scenarios across 6 categories
- 3-turn conversations with each model
- Independent evaluation using DeepSeek-V3 (temperature=0)
- 3 evaluations per conversation, results averaged
- Scoring based on ICF Core Competencies
- Focus on reflective questioning vs advice-giving

## Important Limitations

1. **Not evaluating actual coaching**: This measures coaching-style communication techniques, not embodied coaching relationships
2. **Text-only evaluation**: Cannot assess presence, intuition, or relational dynamics
3. **Simulated conversations**: Turn 3 responses are AI-generated, not from real humans
4. **Specific scenarios**: Results may vary with different types of challenges or conversation contexts

---

*Generated by LLM Reflective Questioning Benchmark*
